# Databricks notebook source
# MAGIC %md
# MAGIC #### In this notebook we will show the steps on the fastest and quickest way of getting data out of SAP Datasphere using SAP FedML libraries.
# MAGIC #### SAP FedML github repo link: 
# MAGIC https://github.com/SAP-samples/datasphere-fedml
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 1: Install fedml package from PyPi repository

# COMMAND ----------

# MAGIC %pip install pandas==1.5.3  fedml-databricks --no-cache-dir --upgrade --force-reinstall

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 2: Restart the Python process on Databricks

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 3: Configure the connection to SAP Datasphere using the following JSON. In order to configure you need the following information
# MAGIC
# MAGIC 	Address: Address of SAP Datasphere
# MAGIC 	Port: Port(443)
# MAGIC 	User: Database user
# MAGIC 	Password: Provide the password the System user
# MAGIC 	Schema: The schema you would like to connect in Datasphere.
# MAGIC

# COMMAND ----------

config_json={
    "address":  "20.51.XXX.XX",
    "port": "443",
    "user": "XXXX",
    "password": "XXXXXXXXXXX",
    "schema": "XXXXXX"
}

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 4: Import the required libraries

# COMMAND ----------

from fedml_databricks import DbConnection

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 5: Create the Dbconnection using the config_json which carries the SAP Datasphere connection information.

# COMMAND ----------

db = DbConnection(dict_obj=config_json)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 6: Check on the API provided by FedML using the help

# COMMAND ----------

help(db)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 7: List all the business models/views avaliable to read from

# COMMAND ----------

data= db.get_schema_views()
data

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 8: Start creating the dataframes using the execute_query_pyspark API from FedML. Here we are bringing in VBAP table from ECC_DATA schema.

# COMMAND ----------

df_sap_ecc_hana_vbap = db.execute_query_pyspark('SELECT * FROM "ECC_DATA"."VBAP"')

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 9: Once the dataframes are created, start using the Databrick's unlimited potential for EDA and ML

# COMMAND ----------

dbutils.data.summarize(df_sap_ecc_hana_vbap)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 10: After the EDA and ML analysis has been performed on the the dataframes. The inference result can be stored back in SAP Datasphere.
# MAGIC #### Using create_table API call we can create a table in SAP Datasphere. 

# COMMAND ----------

db.create_table("CREATE TABLE DS_VBAP_INF (CUSTOMERNAME Varchar(20), PRODUCTLINE Varchar(20), STATUS Varchar(20), PRODUCTCODE Varchar(20), SALES FLOAT, Predicted_SALES FLOAT)")

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 11: Write the prediction results to 'DS_VBAP_INF' table in SAP Datasphere

# COMMAND ----------

db.insert_into_table('DS_VBAP_INF', ml_output_dataframe)